

#' List Engines
#'
#' Lists the currently available engines, and provides basic information about each one such as the owner and availability.
#'
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/engines/list}{Open AI Documentation}
#' @export
list_engines <- function(return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/engines')

	body <- NULL

	query <- NULL

	response <- httr::GET(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Retrieve Engine
#'
#' Retrieves an engine instance, providing basic information about the engine such as the owner and availability.
#'
#' @param engine_id (string) The ID of the engine to use for this request Required
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/engines/retrieve}{Open AI Documentation}
#' @export
retrieve_engine <- function(engine_id, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/engines/{engine_id}')

	body <- NULL

	query <- NULL

	response <- httr::GET(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Create Completion
#'
#' Creates a new completion for the provided prompt and parameters
#'
#' @param engine_id (string) The ID of the engine to use for this request Required
#' @param best_of (integer) Generates best_of completions server-side and returns the "best" (the one with the lowest log probability per token). Results cannot be streamed. When used with n, best_of controls the number of candidate completions and n specifies how many to return â€“ best_of must be greater than n. Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. 
#' @param echo (boolean) Echo back the prompt in addition to the completion 
#' @param frequency_penalty (number) Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. See more information about frequency and presence penalties. 
#' @param logit_bias (map) Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {"50256": -100} to prevent the <|endoftext|> token from being generated. 
#' @param logprobs (integer) Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 10, the API will return a list of the 10 most likely tokens. the API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response. 
#' @param max_tokens (integer) The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except davinci-codex, which supports 4096). 
#' @param n (integer) How many completions to generate for each prompt. Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. 
#' @param presence_penalty (number) Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. See more information about frequency and presence penalties. 
#' @param prompt (string or array) The prompt(s) to generate completions for, encoded as a string, a list of strings, or a list of token lists. Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. 
#' @param stop (string or array) Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. 
#' @param stream (boolean) Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. 
#' @param temperature (number) What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. We generally recommend altering this or top_p but not both. 
#' @param top_p (number) An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. 
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/completions/create}{Open AI Documentation}
#' @export
create_completion <- function(engine_id, best_of = NULL, echo = NULL, frequency_penalty = NULL, logit_bias = NULL, logprobs = NULL, max_tokens = NULL, n = NULL, presence_penalty = NULL, prompt = NULL, stop = NULL, stream = NULL, temperature = NULL, top_p = NULL, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/engines/{engine_id}/completions')

	body_params <- c("best_of","echo","frequency_penalty","logit_bias","logprobs","max_tokens","n","presence_penalty","prompt","stop","stream","temperature","top_p")
	body <- body_params %>% purrr::map(~ eval(parse(text = .x))) %>% setNames(body_params) %>% purrr::compact()

	query <- NULL

	response <- httr::POST(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Create Search
#'
#' The search endpoint computes similarity scores between provided query and documents. Documents can be passed directly to the API if there are no more than 200 of them.
#'
#' @param engine_id (string) The ID of the engine to use for this request Required
#' @param query (string) Query to search against the documents. Required
#' @param documents (array) Up to 200 documents to search over, provided as a list of strings. The maximum document length (in tokens) is 2034 minus the number of tokens in the query. You should specify either documents or a file, but not both. 
#' @param file (string) The ID of an uploaded file that contains documents to search over. You should specify either documents or a file, but not both. 
#' @param max_rerank (integer) The maximum number of documents to be re-ranked and returned by search. This flag only takes effect when file is set. 
#' @param return_metadata (boolean) A special boolean flag for showing metadata. If set to true, each document entry in the returned JSON will contain a "metadata" field. This flag only takes effect when file is set. 
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/searches/create}{Open AI Documentation}
#' @export
create_search <- function(engine_id, query, documents = NULL, file = NULL, max_rerank = NULL, return_metadata = NULL, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/engines/{engine_id}/search')

	body_params <- c("query","documents","file","max_rerank","return_metadata")
	body <- body_params %>% purrr::map(~ eval(parse(text = .x))) %>% setNames(body_params) %>% purrr::compact()

	query <- NULL

	response <- httr::POST(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Create Classification - Beta
#'
#' Classifies the specified query using provided examples.
#'
#' @param model (string) ID of the engine to use for completion. Required
#' @param query (string) Query to be classified. Required
#' @param examples (array) A list of examples with labels, in the following format: [["The movie is so interesting.", "Positive"], ["It is quite boring.", "Negative"], ...] All the label strings will be normalized to be capitalized. You should specify either examples or file, but not both. 
#' @param expand (array) If an object name is in the list, we provide the full information of the object; otherwise, we only provide the object ID. Currently we support completion and file objects for expansion. 
#' @param file (string) The ID of the uploaded file that contains training examples. See upload file for how to upload a file of the desired format and purpose. You should specify either examples or file, but not both. 
#' @param labels (array) The set of categories being classified. If not specified, candidate labels will be automatically collected from the examples you provide. All the label strings will be normalized to be capitalized. 
#' @param logit_bias (map) Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {"50256": -100} to prevent the <|endoftext|> token from being generated. 
#' @param logprobs (integer) Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 10, the API will return a list of the 10 most likely tokens. the API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response. When logprobs is set, completion will be automatically added into expand to get the logprobs. 
#' @param max_examples (integer) The maximum number of examples to be ranked by Search when using file. Setting it to a higher value leads to improved accuracy but with increased latency and cost. 
#' @param return_metadata (boolean) A special boolean flag for showing metadata. If set to true, each document entry in the returned JSON will contain a "metadata" field. This flag only takes effect when file is set. 
#' @param return_prompt (boolean) If set to true, the returned JSON will include a "prompt" field containing the final prompt that was used to request a completion. This is mainly useful for debugging purposes. 
#' @param search_model (string) ID of the engine to use for Search. 
#' @param temperature (number) What sampling temperature to use. Higher values mean the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. 
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/classifications/create}{Open AI Documentation}
#' @export
create_classification <- function(model, query, examples = NULL, expand = NULL, file = NULL, labels = NULL, logit_bias = NULL, logprobs = NULL, max_examples = NULL, return_metadata = NULL, return_prompt = NULL, search_model = NULL, temperature = NULL, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/classifications')

	body_params <- c("model","query","examples","expand","file","labels","logit_bias","logprobs","max_examples","return_metadata","return_prompt","search_model","temperature")
	body <- body_params %>% purrr::map(~ eval(parse(text = .x))) %>% setNames(body_params) %>% purrr::compact()

	query <- NULL

	response <- httr::POST(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Create Answer - Beta
#'
#' Answers the specified question using the provided documents and examples.
#'
#' @param examples (array) List of (question, answer) pairs that will help steer the model towards the tone and answer format you'd like. We recommend adding 2 to 3 examples. Required
#' @param examples_context (string) A text snippet containing the contextual information used to generate the answers for the examples you provide. Required
#' @param model (string) ID of the engine to use for completion. Required
#' @param question (string) Question to get answered. Required
#' @param documents (array) List of documents from which the answer for the input question should be derived. If this is an empty list, the question will be answered based on the question-answer examples. You should specify either documents or a file, but not both. 
#' @param expand (array) If an object name is in the list, we provide the full information of the object; otherwise, we only provide the object ID. Currently we support completion and file objects for expansion. 
#' @param file (string) The ID of an uploaded file that contains documents to search over. See upload file for how to upload a file of the desired format and purpose. You should specify either documents or a file, but not both. 
#' @param logit_bias (map) Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {"50256": -100} to prevent the <|endoftext|> token from being generated. 
#' @param logprobs (integer) Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 10, the API will return a list of the 10 most likely tokens. the API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response. When logprobs is set, completion will be automatically added into expand to get the logprobs. 
#' @param max_rerank (integer) The maximum number of documents to be ranked by Search when using file. Setting it to a higher value leads to improved accuracy but with increased latency and cost. 
#' @param max_tokens (integer) The maximum number of tokens allowed for the generated answer 
#' @param n (integer) How many answers to generate for each question. 
#' @param return_metadata (boolean) A special boolean flag for showing metadata. If set to true, each document entry in the returned JSON will contain a "metadata" field. This flag only takes effect when file is set. 
#' @param return_prompt (boolean) If set to true, the returned JSON will include a "prompt" field containing the final prompt that was used to request a completion. This is mainly useful for debugging purposes. 
#' @param search_model (string) ID of the engine to use for Search. 
#' @param stop (string or array) Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. 
#' @param temperature (number) What sampling temperature to use. Higher values mean the model will take more risks and value 0 (argmax sampling) works better for scenarios with a well-defined answer. 
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/answers/create}{Open AI Documentation}
#' @export
create_answer <- function(examples, examples_context, model, question, documents = NULL, expand = NULL, file = NULL, logit_bias = NULL, logprobs = NULL, max_rerank = NULL, max_tokens = NULL, n = NULL, return_metadata = NULL, return_prompt = NULL, search_model = NULL, stop = NULL, temperature = NULL, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/answers')

	body_params <- c("examples","examples_context","model","question","documents","expand","file","logit_bias","logprobs","max_rerank","max_tokens","n","return_metadata","return_prompt","search_model","stop","temperature")
	body <- body_params %>% purrr::map(~ eval(parse(text = .x))) %>% setNames(body_params) %>% purrr::compact()

	query <- NULL

	response <- httr::POST(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' List Files
#'
#' Returns a list of files that belong to the user's organization.
#'
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/files/list}{Open AI Documentation}
#' @export
list_files <- function(return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/files')

	body <- NULL

	query <- NULL

	response <- httr::GET(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Upload File
#'
#' Upload a file that contains document(s) to be used across various endpoints/features. Currently, the size of all the files uploaded by one organization can be up to 1 GB. Please contact us if you need to increase the storage limit.
#'
#' @param file (string) Name of the JSON Lines file to be uploaded. If the purpose is set to "search" or "answers", each line is a JSON record with a "text" field and an optional "metadata" field. Only "text" field will be used for search. Specially, when the purpose is "answers", "(newline character)" is used as a delimiter to chunk contents in the "text" field into multiple documents for finer-grained matching. If the purpose is set to "classifications", each line is a JSON record representing a single training example with "text" and "label" fields along with an optional "metadata" field. If the purpose is set to "fine-tune", each line is a JSON record with "prompt" and "completion" fields representing your training examples. Required
#' @param purpose (string) The intended purpose of the uploaded documents. Use "search" for Search, "answers" for Answers, "classifications" for Classifications and "fine-tune" for Fine-tuning. This allows us to validate the format of the uploaded file. Required
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/files/upload}{Open AI Documentation}
#' @export
upload_file <- function(file, purpose, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/files')

	body_params <- c("file","purpose")
	body <- body_params %>% purrr::map(~ eval(parse(text = .x))) %>% setNames(body_params) %>% purrr::compact()

	body$file <- httr::upload_file(body$file)

	query <- NULL

	response <- httr::POST(url = endpoint_url, body = body, encode = 'multipart', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Delete File
#'
#' Delete a file.
#'
#' @param file_id (string) The ID of the file to use for this request Required
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/files/delete}{Open AI Documentation}
#' @export
delete_file <- function(file_id, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/files/{file_id}')

	body <- NULL

	query <- NULL

	response <- httr::DELETE(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Retrieve File
#'
#' Returns information about a specific file.
#'
#' @param file_id (string) The ID of the file to use for this request Required
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/files/retrieve}{Open AI Documentation}
#' @export
retrieve_file <- function(file_id, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/files/{file_id}')

	body <- NULL

	query <- NULL

	response <- httr::GET(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Retrieve File Content
#'
#' Returns the contents of the specified file
#'
#' @param file_id (string) The ID of the file to use for this request Required
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/files/retrieve-content}{Open AI Documentation}
#' @export
retrieve_file_content <- function(file_id, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/files/{file_id}/content')

	body <- NULL

	query <- NULL

	response <- httr::GET(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Create Fine-Tune - Beta
#'
#' Creates a job that fine-tunes a specified model from a given dataset.
#'
#' @param training_file (string) The ID of an uploaded file that contains training data. See upload file for how to upload a file. Your dataset must be formatted as a JSONL file, where each training example is a JSON object with the keys "prompt" and "completion". Additionally, you must upload your file with the purpose fine-tune. See the fine-tuning guide for more details. Required
#' @param batch_size (integer) The batch size to use for training. The batch size is the number of training examples used to train a single forward and backward pass. When use_packing is true, the batch size becomes the number of 2048-token contexts instead of the number of raw examples (see below for details on packing). By default, the batch size will be dynamically configured to be ~0.2% of the training set, capped at 8 - in general, we've found that larger batch sizes tend to work better for larger datasets. 
#' @param classification_betas (array) If this is provided, we calculate F-beta scores at the specified beta values. The F-beta score is a generalization of F-1 score. This is only used for binary classification. With a beta of 1 (i.e. the F-1 score), precision and recall are given the same weight. A larger beta score puts more weight on recall and less on precision. A smaller beta score puts more weight on precision and less on recall. 
#' @param classification_n_classes (integer) The number of classes in a classification task. This parameter is required for multiclass classification. 
#' @param classification_positive_class (string) The positive class in binary classification. This parameter is needed to generate precision, recall, and F1 metrics when doing binary classification. 
#' @param compute_classification_metrics (boolean) If set, we calculate classification-specific metrics such as accuracy and F-1 score using the validation set at the end of every epoch. These metrics can be viewed in the results file. In order to compute classification metrics, you must provide a validation_file. Additionally, you must specify classification_n_classes for multiclass classification or classification_positive_class for binary classification. 
#' @param learning_rate_multiplier (number) The learning rate multiplier to use for training. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this value. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. With larger batch sizes, larger learning rates tend to perform better. 
#' @param model (string) The name of the base model to fine-tune. You can select one of "ada", "babbage", or "curie". To learn more about these models, see the Engines documentation. 
#' @param n_epochs (integer) The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. 
#' @param prompt_loss_weight (number) The weight to use for loss on the prompt tokens. This controls how much the model tries to learn to generate the prompt (as compared to the completion which always has a weight of 1.0), and can add a stabilizing effect to training when completions are short. If prompts are extremely long (relative to completions), it may make sense to reduce this weight so as to avoid over-prioritizing learning the prompt. 
#' @param use_packing (boolean) On classification tasks and small datasets, we recommend setting this to false. On all other tasks, we recommend setting this to true. When true, we pack as many prompt-completion pairs as possible into each training example. This greatly increases the speed of a fine-tuning job, often without negatively affecting model performance. In particular, with packing, each example in a training batch takes the form <prompt1><completion1><end_token><prompt2><completion2><end_token>.... Without packing, each example contains a single prompt-completion pair. By default, use_packing is true for datasets with at least 500k total tokens. 
#' @param validation_file (string) The ID of an uploaded file that contains validation data. If you provide this file, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in the fine-tuning results file. Your train and validation data should be mutually exclusive. Your dataset must be formatted as a JSONL file, where each validation example is a JSON object with the keys "prompt" and "completion". Additionally, you must upload your file with the purpose fine-tune. See the fine-tuning guide for more details. 
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/fine-tunes/create}{Open AI Documentation}
#' @export
create_fine_tune <- function(training_file, batch_size = NULL, classification_betas = NULL, classification_n_classes = NULL, classification_positive_class = NULL, compute_classification_metrics = NULL, learning_rate_multiplier = NULL, model = NULL, n_epochs = NULL, prompt_loss_weight = NULL, use_packing = NULL, validation_file = NULL, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/fine-tunes')

	body_params <- c("training_file","batch_size","classification_betas","classification_n_classes","classification_positive_class","compute_classification_metrics","learning_rate_multiplier","model","n_epochs","prompt_loss_weight","use_packing","validation_file")
	body <- body_params %>% purrr::map(~ eval(parse(text = .x))) %>% setNames(body_params) %>% purrr::compact()

	query <- NULL

	response <- httr::POST(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' List Fine-Tunes - Beta
#'
#' List your organization's fine-tuning jobs
#'
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/fine-tunes/list}{Open AI Documentation}
#' @export
list_fine_tunes <- function(return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/fine-tunes')

	body <- NULL

	query <- NULL

	response <- httr::GET(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Retrieve Fine-Tune - Beta
#'
#' Gets info about the fine-tune job.
#'
#' @param fine_tune_id (string) The ID of the fine-tune job Required
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/fine-tunes/retrieve}{Open AI Documentation}
#' @export
retrieve_fine_tune <- function(fine_tune_id, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/fine-tunes/{fine_tune_id}')

	body <- NULL

	query <- NULL

	response <- httr::GET(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' Cancel Fine-Tune - Beta
#'
#' Immediately cancel a fine-tune job.
#'
#' @param fine_tune_id (string) The ID of the fine-tune job to cancel Required
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/fine-tunes/cancel}{Open AI Documentation}
#' @export
cancel_fine_tune <- function(fine_tune_id, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/fine-tunes/{fine_tune_id}/cancel')

	body <- NULL

	query <- NULL

	response <- httr::POST(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



#' List Fine-Tune Events - Beta
#'
#' Get fine-grained status updates for a fine-tune job.
#'
#' @param fine_tune_id (string) The ID of the fine-tune job to get events for. Required
#' @param stream (boolean) Whether to stream events for the fine-tune job. If set to true, events will be sent as data-only server-sent events as they become available. The stream will terminate with a data: [DONE] message when the job is finished (succeeded, cancelled, or failed). If set to false, only events generated so far will be returned. 
#' @param return_response (boolean) Whether to return the API response or parse the contents of the response. Defaults to FALSE (parse the response).
#' @seealso \href{https://beta.openai.com/docs/api-reference/fine-tunes/events}{Open AI Documentation}
#' @export
list_fine_tune_events <- function(fine_tune_id, stream = NULL, return_response = F){

	check_authentication()

	endpoint_url <- glue::glue('https://api.openai.com/v1/fine-tunes/{fine_tune_id}/events')

	body <- NULL

	query_params <- c("stream")
	query <- query_params %>% purrr::map(~ eval(parse(text = .x))) %>% setNames(query_params) %>% purrr::compact()

	response <- httr::GET(url = endpoint_url, body = body, encode = 'json', query = query, httr::add_headers(`OpenAI-Organization` = Sys.getenv("openai_organization_id"), Authorization = glue::glue('Bearer {Sys.getenv("openai_secret_key")}')))

	if(return_response) return(response)

	parse_response(response)

}



